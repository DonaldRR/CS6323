import osimport torchimport numpy as npfrom tensorboardX import SummaryWriterfrom config import *from utils import *class Buffer:    def __init__(self, buffer_size=5000, batch_size=16):        self.buffer_size = buffer_size        self.batch_size = batch_size        self.memory = []        self.W = []        self.memory_tracks = []    def append_track(self, history, rewards):        # track = [s_0, a_0, r_0, s_1, a_1, r_1, ..., s_T-1, a_T-1, r_T-1, s_T]        # Extract history, h as [[a_-1, s_0], [a_0, s_1], ..., [a_T-1, s_T]] adding null action a_-1        # Extrack rewards [r_0, r_1, ..., r_T-1]        # y = r_t + gamma * Q(h[:t+2], A(h[:t+2))        # Q(h[:t+1], a_t) == y        # result, [[h[:1], a_0, r_0, h[:2]], [h[:2], a_1, r_1, h[:3]], ... , [h[:T], a_T-1, r_T-1, h[:T+1]]]        #if len(self.memory_tracks) > self.buffer_size:        #    self.memory_tracks.pop(0)        T = len(rewards)        Track = []        for t in range(T):            Track.append([history[:t+1, :], history[t, :N_PREDATOR * 2], rewards[t], history[:t+2, :]])        self.memory_tracks.append(Track)    def clear_tracks(self):        self.memory_tracks = []    def save_tracks(self, save_path):        save_doc = []        for track in self.memory_tracks:            tmp_dict = {}            for t in range(len(track)):                tmp_dict[t] = {                    'h_t':track[t][0].numpy(),                    'a_t': track[t][1].numpy(),                    'r_t': track[t][2].numpy(),                    'h_t_': track[t][3].numpy(),                }            save_doc.append(tmp_dict)        np.save(save_path, save_doc, allow_pickle=True)    def load_tracks(self, load_path):        self.clear_tracks()        tracks = np.load(load_path, allow_pickle=True).item()        for track in range(tracks):            tmp_track = []            for t in range(len(track)):                tmp_track.append([                    toTensor(track[t]['h_t']),                    toTensor(track[t]['a_t']),                    toTensor(track[t]['r_t']),                    toTensor(track[t]['h_t_'])                ])            self.memory_tracks.append(tmp_track)    def get_batch_tracks(self):        H_t = []        A_t = []        R_t = []        H_t_ = []        mask_t = []        mask_t_ = []        idxs = np.random.choice(list(range(len(self.memory_tracks))), self.batch_size)        for idx in idxs:            sample_track = self.memory_tracks[idx]            for t in range(len(sample_track)):                seq_len = sample_track[t][0].size()[0]                H_t.append(torch.nn.functional.pad(sample_track[t][0], (0, 0, 0, SEQUENCE_LEN - seq_len), "constant", 0))                A_t.append(sample_track[t][1])                R_t.append(sample_track[t][2])                H_t_.append(torch.nn.functional.pad(sample_track[t][3], (0, 0, 0, SEQUENCE_LEN - seq_len - 1), "constant", 0))                mask_t.append(torch.zeros(SEQUENCE_LEN))                mask_t_.append(torch.zeros(SEQUENCE_LEN))                mask_t[-1][seq_len - 1] = 1                mask_t_[-1][seq_len] = 1        H_t = torch.stack(H_t, dim=0)        A_t = torch.stack(A_t, dim=0)        R_t = torch.stack(R_t, dim=0)        H_t_ = torch.stack(H_t_, dim=0)        mask_t = torch.stack(mask_t, dim=0)        mask_t_ = torch.stack(mask_t_, dim=0)        return H_t, A_t, R_t, H_t_, mask_t, mask_t_    def append(self, w, l):        """        :param l: [state_t, action_t, reward_t, state_t+1]        :return:        """        if len(self.memory) > self.buffer_size:            self.memory.pop(0)            self.W.pop(0)        self.memory.append(l)        self.W.append(w)    def normalize_weights(self):        norm_W = np.array(self.W) / np.sum(self.W)        return norm_W    def get_batch(self):        idxs = np.random.choice(list(range(len(self.memory))), self.batch_size, p=self.normalize_weights())        s_t, a_t, r_t, s_t_ = [], [], [], []        for idx in idxs:            s_t.append(self.memory[idx][0])            a_t.append(self.memory[idx][1])            r_t.append(self.memory[idx][2])            s_t_.append(self.memory[idx][3])        s_t = torch.stack(tuple(s_t), dim=0)        a_t = torch.stack(tuple(a_t), dim=0)        r_t = torch.stack(tuple(r_t), dim=0)        s_t_ = torch.stack(tuple(s_t_), dim=0)        return s_t, a_t, r_t, s_t_    @property    def ready(self):        return len(self.memory) >= self.batch_size    def statistics(self):        W = np.array(self.W)        w_p = np.sum(W[W >= 1]) / np.sum(W)        n_p = np.sum(W >= 1) / len(W)        #print('positive_weights:%.4f positive_num:%.4f' %  (w_p, n_p))        return w_p, n_pclass Message:    def __init__(self, bytes):        self.preys = None        self.predators = None        self.termination = None        self.action = None        self.decode(bytes)    def decode(self, input):        """        message format:        """        input = input.decode('utf_8')        input_list = input.split("\n")[:-1]        self.preys = []        self.predators = []        for i in range(len(input_list) - 2):            t = input_list[i].split(';')            type = int(t[0].split(':')[1])            pos = t[1].split(':')[1][1:-1]            pos = list(map(float, pos.split(',')))            speed = t[2].split(':')[1][1:-1]            speed = list(map(float, speed.split(',')))            tmp = {                'type': type,                'pos': pos,                'speed': speed            }            if type == 0:                self.preys.append(tmp)            else:                self.predators.append(tmp)        self.action = convert_nameValue(input_list[-2])        self.termination = convert_nameValue(input_list[-1])    @property    def is_terminal(self):        return self.termination == 1    @property    def require_action(self):        return self.action == 1    @property    def state(self):        state = []        for _, agent_state in enumerate(self.predators):            state.extend(agent_state['pos'])            state.extend(agent_state['speed'])        for _, agent_state in enumerate(self.preys):            state.extend(agent_state['pos'])            state.extend(agent_state['speed'])        return state    @property    def states(self):        states = []        for i, agent_state in enumerate(self.predators):            state = []            state.append(agent_state['pos'])            state.append(agent_state['speed'])            for j, agent_state_ in enumerate(self.predators):                if i != j:                    state.append(agent_state_['pos'])                    state.append(agent_state_['speed'])            for j, agent_state_ in enumerate(self.preys):                state.append(agent_state_['pos'])                state.append(agent_state_['speed'])            states.append(state)        return statesclass Logger:    def __init__(self, log_dir):        self.log_dir = log_dir        self.logger = None        self.clear_log()        self.init_logger()    def clear_log(self):        if os.path.exists(self.log_dir):            os.system('rm -r %s' % self.log_dir)    def init_logger(self):        self.logger = SummaryWriter(self.log_dir)    @property    def writer(self):        return self.loggerclass LRScheduler:    def __init__(self, init_lr, max_iters):        self.init_lr = init_lr        self.max_iters = max_iters        self.global_t = 0    def step(self):        lr = self.init_lr * (1 - self.global_t / self.max_iters)**1.2        self.global_t += 1        return lrclass NoiseScheduler:    def __init__(self, init_noise, min_noise, decay_T):        self.init_noise = init_noise        self.decay_T = decay_T        self.min_noise = min_noise        self.global_t = 0    def step(self):        noise = self.init_noise * (1 - self.global_t / self.decay_T)**1.2 + self.min_noise        self.global_t += 1        return noise